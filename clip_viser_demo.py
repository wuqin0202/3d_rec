#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
CLIP_Surgeryè¯­è¨€å¼•å¯¼3Dé«˜äº®åŠŸèƒ½å®ç°

åŸºäºCLIP_Surgeryæ¨¡å‹çš„3Dç‚¹äº‘é«˜äº®æŸ¥è¯¢ç³»ç»Ÿã€‚
ç›¸æ¯”åŸç‰ˆdemo_clip_highlight.pyï¼Œæ­¤ç‰ˆæœ¬ä½¿ç”¨CLIP_Surgeryç›´æ¥æå–patchçº§ç‰¹å¾ï¼Œ
é¿å…äº†ç½‘æ ¼åˆ‡åˆ†çš„å¤šæ¬¡æ¨ç†è¿‡ç¨‹ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ è½½VGGTé‡å»ºç»“æœ
2. ä½¿ç”¨CLIP_Surgeryé¢„è®¡ç®—å›¾åƒpatchç‰¹å¾
3. å¯åŠ¨ViseræœåŠ¡å™¨å¹¶é›†æˆé«˜äº®åŠŸèƒ½
4. æä¾›äº¤äº’å¼è¯­è¨€æŸ¥è¯¢ç•Œé¢

ä½¿ç”¨æ–¹æ³•ï¼š
python main.py --data_dir <é‡å»ºæ•°æ®ç›®å½•>

ä½œè€…ï¼šVGGT Team
"""

import os
import sys
import argparse
import time
import threading
import numpy as np
import torch
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging
from PIL import Image
import gc

# å¯¼å…¥å¿…è¦æ¨¡å—
import viser
import viser.transforms
from vggt.models.vggt import VGGT
from vggt.utils.load_fn import load_and_preprocess_images
from vggt.utils.geometry import closed_form_inverse_se3, unproject_depth_map_to_point_map

# å¯¼å…¥CLIP_Surgery
import clip


class ClipSurgeryFeatureManager:
    """CLIP_Surgeryç‰¹å¾ç®¡ç†å™¨ - æ ¸å¿ƒç±»"""

    def __init__(self, model_name: str = "CS-ViT-B/16"):
        """
        åˆå§‹åŒ–CLIP_Surgeryç‰¹å¾ç®¡ç†å™¨

        Args:
            model_name: CLIP_Surgeryæ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.similarity_threshold = 0.25
        self.highlight_color = [255, 255, 0]
        self.point_size = 0.003

        # ä¸‹é‡‡æ ·é…ç½®
        self.enable_downsampling = True
        self.max_points = 50000
        self.downsample_method = 'uniform'

        # åˆå§‹åŒ–æ¨¡å‹
        self.device = self._get_device()
        self.clip_model = None
        self.clip_preprocess = None
        self._load_clip_surgery_model()

        # ç‰¹å¾ç¼“å­˜
        self.feature_cache = {}
        self.scene_hash = None

        print(f"âœ… ClipSurgeryFeatureManageråˆå§‹åŒ–å®Œæˆ")
        print(f"   æ¨¡å‹: {self.model_name}")
        print(f"   è®¾å¤‡: {self.device}")

    def _get_device(self) -> str:
        """è·å–è®¡ç®—è®¾å¤‡"""
        if torch.cuda.is_available():
            visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', None)
            if visible_devices is not None:
                device = "cuda:0"
            else:
                device = "cuda"

            current_device = torch.cuda.current_device()
            device_name = torch.cuda.get_device_name(current_device)
            print(f"ğŸš€ CLIP_Surgeryä½¿ç”¨GPUè®¾å¤‡: {device} ({device_name})")
            return device
        else:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼ŒCLIP_Surgeryä½¿ç”¨CPU")
            return "cpu"

    def _load_clip_surgery_model(self):
        """åŠ è½½CLIP_Surgeryæ¨¡å‹"""
        try:
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½CLIP_Surgeryæ¨¡å‹: {self.model_name}")
            start_time = time.time()

            self.clip_model, self.clip_preprocess = clip.load(self.model_name, device=self.device)
            self.clip_model.eval()

            load_time = time.time() - start_time
            print(f"âœ… CLIP_Surgeryæ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}s)")

        except Exception as e:
            print(f"âŒ CLIP_Surgeryæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def _generate_scene_hash(self, predictions: Dict[str, Any]) -> str:
        """ç”Ÿæˆåœºæ™¯çš„å”¯ä¸€å“ˆå¸Œå€¼"""
        try:
            import hashlib

            # ä½¿ç”¨å›¾åƒæ•°æ®å’Œç›¸æœºå‚æ•°ç”Ÿæˆå“ˆå¸Œ
            images = predictions['images']
            extrinsics = predictions['extrinsic']
            intrinsics = predictions['intrinsic']

            # åˆ›å»ºå“ˆå¸Œè¾“å…¥
            hash_input = []
            hash_input.append(images.shape)
            hash_input.append(np.mean(images.cpu().numpy() if hasattr(images, 'cpu') else images))
            hash_input.append(extrinsics.flatten())
            hash_input.append(intrinsics.flatten())

            # ç”ŸæˆMD5å“ˆå¸Œ
            hash_str = str(hash_input).encode('utf-8')
            scene_hash = hashlib.md5(hash_str).hexdigest()[:16]

            return scene_hash

        except Exception as e:
            print(f"âš ï¸ åœºæ™¯å“ˆå¸Œç”Ÿæˆå¤±è´¥: {e}")
            return f"scene_{int(time.time())}"

    def precompute_features(self, predictions: Dict[str, Any]) -> str:
        """
        é¢„è®¡ç®—é˜¶æ®µï¼šä½¿ç”¨CLIP_Surgeryæå–å›¾åƒpatchç‰¹å¾å¹¶å»ºç«‹3D-2Dç´¢å¼•

        Args:
            predictions: VGGTæ¨¡å‹é¢„æµ‹ç»“æœ

        Returns:
            åœºæ™¯å“ˆå¸Œå€¼
        """
        print("ğŸš€ å¼€å§‹CLIP_Surgeryç‰¹å¾é¢„è®¡ç®—...")
        start_time = time.time()

        # ç”Ÿæˆåœºæ™¯å“ˆå¸Œ
        print("ğŸ“ æ­¥éª¤ 1/3: ç”Ÿæˆåœºæ™¯å“ˆå¸Œ...")
        scene_hash = self._generate_scene_hash(predictions)
        self.scene_hash = scene_hash
        print(f"ğŸ¯ åœºæ™¯å“ˆå¸Œ: {scene_hash}")

        # æå–å›¾åƒpatchç‰¹å¾
        print("ğŸ–¼ï¸ æ­¥éª¤ 2/3: æå–CLIP_Surgery patchç‰¹å¾...")
        feature_start_time = time.time()
        image_features = self._extract_clip_surgery_features(predictions['images'])
        feature_time = time.time() - feature_start_time
        print(f"âœ… å›¾åƒç‰¹å¾æå–å®Œæˆ ({feature_time:.2f}s)")

        # å»ºç«‹3D-2Dæ˜ å°„ç´¢å¼•
        print("ğŸ—ºï¸ æ­¥éª¤ 3/3: å»ºç«‹3D-2D patchæ˜ å°„ç´¢å¼•...")
        mapping_start_time = time.time()
        point_features = self._build_3d_2d_patch_mapping(predictions, image_features)
        mapping_time = time.time() - mapping_start_time
        print(f"âœ… 3D-2Dæ˜ å°„å®Œæˆ ({mapping_time:.2f}s)")

        # ä¸‹é‡‡æ ·ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
        if self.enable_downsampling:
            print("ğŸ”½ é¢å¤–æ­¥éª¤: ä¸‹é‡‡æ ·3Dç‚¹ä»¥æé«˜æ€§èƒ½...")
            downsample_start_time = time.time()
            point_features = self._downsample_point_features(
                point_features,
                max_points=self.max_points,
                method=self.downsample_method
            )
            downsample_time = time.time() - downsample_start_time
            print(f"âœ… ä¸‹é‡‡æ ·å®Œæˆ ({downsample_time:.2f}s)")

        # ä¿å­˜ç‰¹å¾
        features = {
            'scene_hash': scene_hash,
            'point_features': point_features,
            'image_features': image_features,
            'predictions_meta': {
                'num_views': predictions['images'].shape[0],
                'image_shape': predictions['images'].shape[2:],
                'device': str(predictions['images'].device) if hasattr(predictions['images'], 'device') else 'cpu'
            }
        }

        # ç¼“å­˜ç‰¹å¾
        self.feature_cache = features

        total_time = time.time() - start_time
        num_points = len(point_features)
        print(f"âœ… CLIP_Surgeryç‰¹å¾é¢„è®¡ç®—å®Œæˆ: {num_points} ä¸ª3Dç‚¹ ({total_time:.2f}s)")

        return scene_hash

    def _extract_clip_surgery_features(self, images) -> Dict[int, Dict[str, any]]:
        """
        ä½¿ç”¨CLIP_Surgeryæå–patchçº§ç‰¹å¾

        Args:
            images: å›¾åƒæ•°æ® (S, 3, H, W)

        Returns:
            æ¯ä¸ªè§†å›¾çš„patchç‰¹å¾å­—å…¸ {view_idx: {'patch_features': tensor, 'patch_coords': coords}}
        """
        image_features = {}

        # å¤„ç†ä¸åŒçš„è¾“å…¥ç±»å‹
        if isinstance(images, np.ndarray):
            images = torch.from_numpy(images).float()
        elif isinstance(images, torch.Tensor):
            images = images.cpu().float()

        S, C, H, W = images.shape
        print(f"   ğŸ“Š å¼€å§‹CLIP_Surgeryå¤„ç† {S} ä¸ªè§†å›¾ï¼Œå›¾åƒå°ºå¯¸: {H}x{W}")

        with torch.no_grad():
            for i in range(S):
                print(f"   ğŸ”„ å¤„ç†è§†å›¾ {i+1}/{S}...")

                img = images[i]  # (3, H, W)

                # è½¬æ¢ä¸ºPILå›¾åƒå¹¶é¢„å¤„ç†
                if isinstance(img, torch.Tensor):
                    img_np = (img.permute(1, 2, 0).numpy() * 255).astype(np.uint8)
                else:
                    img_np = (img.transpose(1, 2, 0) * 255).astype(np.uint8)

                pil_img = Image.fromarray(img_np)

                # CLIP_Surgeryé¢„å¤„ç†
                processed_img = self.clip_preprocess(pil_img).unsqueeze(0).to(self.device)

                # ä½¿ç”¨CLIP_Surgeryæå–ç‰¹å¾
                # encode_imageè¿”å›çš„æ˜¯ (B, N, D) å…¶ä¸­ N = HW/patch_size^2 + 1 (åŒ…å«CLS token)
                patch_features = self.clip_model.encode_image(processed_img)  # (1, N, D)

                # ç§»é™¤batchç»´åº¦å’ŒCLS tokenï¼Œåªä¿ç•™patchç‰¹å¾
                patch_features = patch_features[0, 1:, :]  # (N-1, D) ç§»é™¤CLS token

                # è®¡ç®—patch gridçš„å¤§å°
                # å‡è®¾input_resolution=224, patch_size=16, åˆ™æœ‰ (224/16)^2 = 196 ä¸ªpatches
                patch_size = 16 if "16" in self.model_name else 32 if "32" in self.model_name else 14
                input_resolution = 224  # CLIPé»˜è®¤è¾“å…¥å°ºå¯¸
                patches_per_side = input_resolution // patch_size

                print(f"     patchæ•°é‡: {patch_features.shape[0]}, ç‰¹å¾ç»´åº¦: {patch_features.shape[1]}")
                print(f"     patch grid: {patches_per_side}x{patches_per_side}")

                # ç”Ÿæˆpatchåæ ‡
                patch_coords = []
                patch_h = H / patches_per_side
                patch_w = W / patches_per_side

                for patch_idx in range(patch_features.shape[0]):
                    row = patch_idx // patches_per_side
                    col = patch_idx % patches_per_side

                    # è®¡ç®—åœ¨åŸå›¾ä¸­çš„åæ ‡èŒƒå›´
                    y_start = int(row * patch_h)
                    y_end = int((row + 1) * patch_h)
                    x_start = int(col * patch_w)
                    x_end = int((col + 1) * patch_w)

                    patch_coords.append({
                        'patch_idx': patch_idx,
                        'row': row,
                        'col': col,
                        'y_start': y_start,
                        'y_end': y_end,
                        'x_start': x_start,
                        'x_end': x_end,
                        'center_y': (y_start + y_end) // 2,
                        'center_x': (x_start + x_end) // 2
                    })

                image_features[i] = {
                    'patch_features': patch_features,  # (N, D)
                    'patch_coords': patch_coords,      # List[Dict]
                    'patch_grid_shape': (patches_per_side, patches_per_side),
                    'original_shape': (H, W)
                }

                # æ˜¾ç¤ºè¿›åº¦
                progress_percent = ((i + 1) / S) * 100
                print(f"   âœ… è§†å›¾ {i+1}/{S} å®Œæˆ ({progress_percent:.1f}%) - {patch_features.shape[0]} ä¸ªpatches")

                # æ¸…ç†GPUå†…å­˜
                if self.device.startswith('cuda') and (i + 1) % 2 == 0:
                    torch.cuda.empty_cache()

        # æ¸…ç†GPUå†…å­˜
        if self.device.startswith('cuda'):
            torch.cuda.empty_cache()
            gc.collect()

        return image_features

    def _build_3d_2d_patch_mapping(self, predictions: Dict[str, Any], image_features: Dict[int, Dict[str, any]]) -> Dict[str, torch.Tensor]:
        """
        åŸºäºCLIP_Surgery patchesçš„3D-2Dæ˜ å°„ç´¢å¼•å»ºç«‹

        Args:
            predictions: VGGTé¢„æµ‹ç»“æœ
            image_features: patchç‰¹å¾å­—å…¸

        Returns:
            3Dç‚¹ç‰¹å¾å­—å…¸ {point_key: patch_feature_tensor}
        """
        # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
        depth = predictions["depth"]
        extrinsic = predictions["extrinsic"]
        intrinsic = predictions["intrinsic"]

        # è½¬æ¢ä¸ºtorchå¼ é‡ï¼ˆå¦‚æœæ˜¯numpyæ•°ç»„ï¼‰
        if isinstance(depth, np.ndarray):
            depth = torch.from_numpy(depth).float()
        if isinstance(extrinsic, np.ndarray):
            extrinsic = torch.from_numpy(extrinsic).float()
        if isinstance(intrinsic, np.ndarray):
            intrinsic = torch.from_numpy(intrinsic).float()

        # ç”Ÿæˆä¸–ç•Œåæ ‡ç‚¹äº‘
        world_points = unproject_depth_map_to_point_map(depth, extrinsic, intrinsic)

        S, H, W, _ = world_points.shape
        point_features = {}

        print(f"   å¤„ç† {S} ä¸ªè§†å›¾çš„patch 3D-2Dæ˜ å°„ï¼Œå›¾åƒå°ºå¯¸: {H}x{W}")

        # è®¡ç®—æ€»çš„åƒç´ æ•°ç”¨äºè¿›åº¦æ˜¾ç¤º
        total_pixels = S * H * W
        processed_pixels = 0

        for view_idx in range(S):
            if view_idx not in image_features:
                continue

            patch_data = image_features[view_idx]
            patch_features = patch_data['patch_features']  # (N, D)
            patch_coords = patch_data['patch_coords']      # List[Dict]
            patch_grid_shape = patch_data['patch_grid_shape']  # (rows, cols)

            view_world_points = world_points[view_idx]  # (H, W, 3)

            print(f"   è§†å›¾ {view_idx}: patch grid {patch_grid_shape}, å›¾åƒå°ºå¯¸ {H}x{W}")

            # éå†æ¯ä¸ªåƒç´ ï¼Œæ·»åŠ è¿›åº¦æ˜¾ç¤º
            pixels_in_view = H * W
            progress_step = max(1, pixels_in_view // 20)  # æ¯5%æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦

            for y in range(H):
                for x in range(W):
                    # è·å–3Dç‚¹åæ ‡
                    point_3d = view_world_points[y, x]  # (3,)

                    # æ£€æŸ¥ç‚¹çš„æœ‰æ•ˆæ€§
                    if np.isnan(point_3d).any() or np.isinf(point_3d).any():
                        processed_pixels += 1
                        continue

                    # æ‰¾åˆ°å¯¹åº”çš„patch
                    patch_found = False
                    for patch_coord in patch_coords:
                        if (patch_coord['y_start'] <= y < patch_coord['y_end'] and
                            patch_coord['x_start'] <= x < patch_coord['x_end']):

                            # è·å–å¯¹åº”çš„patchç‰¹å¾
                            patch_idx = patch_coord['patch_idx']
                            patch_feature = patch_features[patch_idx]  # (D,)

                            # åˆ›å»º3Dç‚¹çš„é”®ï¼ˆé‡åŒ–åæ ‡ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
                            point_key = f"{point_3d[0]:.3f},{point_3d[1]:.3f},{point_3d[2]:.3f}"

                            # å­˜å‚¨ç‚¹ç‰¹å¾
                            point_features[point_key] = patch_feature
                            patch_found = True
                            break

                    if not patch_found:
                        print(f"   âš ï¸ è­¦å‘Š: åƒç´  ({y}, {x}) æœªæ‰¾åˆ°å¯¹åº”patch")

                    processed_pixels += 1

                    # æ˜¾ç¤ºè¿›åº¦
                    if processed_pixels % progress_step == 0:
                        progress_percent = (processed_pixels / total_pixels) * 100
                        current_view_progress = ((y * W + x + 1) / pixels_in_view) * 100
                        print(f"     è§†å›¾ {view_idx}: {current_view_progress:.1f}% | æ€»è¿›åº¦: {progress_percent:.1f}% | å·²å»ºç«‹ {len(point_features)} ä¸ª3Dç‚¹")

            print(f"   âœ… è§†å›¾ {view_idx} å®Œæˆï¼Œå½“å‰3Dç‚¹æ€»æ•°: {len(point_features)}")

        print(f"   å»ºç«‹äº† {len(point_features)} ä¸ª3Dç‚¹çš„patchç‰¹å¾ç´¢å¼•")

        return point_features

    def _downsample_point_features(self, point_features: Dict[str, torch.Tensor],
                                 max_points: int = 50000,
                                 method: str = "uniform") -> Dict[str, torch.Tensor]:
        """
        ä¸‹é‡‡æ ·3Dç‚¹ç‰¹å¾ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½

        Args:
            point_features: åŸå§‹3Dç‚¹ç‰¹å¾å­—å…¸
            max_points: æœ€å¤§ä¿ç•™ç‚¹æ•°
            method: ä¸‹é‡‡æ ·æ–¹æ³• ("uniform", "random")

        Returns:
            ä¸‹é‡‡æ ·åçš„3Dç‚¹ç‰¹å¾å­—å…¸
        """
        original_count = len(point_features)

        if original_count <= max_points:
            print(f"   ç‚¹æ•° {original_count} å·²åœ¨é™åˆ¶å†…ï¼Œæ— éœ€ä¸‹é‡‡æ ·")
            return point_features

        print(f"   åŸå§‹ç‚¹æ•°: {original_count}, ç›®æ ‡ç‚¹æ•°: {max_points}, æ–¹æ³•: {method}")

        if method == "uniform":
            # å‡åŒ€ä¸‹é‡‡æ ·ï¼šæ¯éš”å›ºå®šé—´éš”é€‰æ‹©ç‚¹
            step = original_count // max_points
            selected_keys = list(point_features.keys())[::step][:max_points]

        elif method == "random":
            # éšæœºä¸‹é‡‡æ ·
            import random
            all_keys = list(point_features.keys())
            selected_keys = random.sample(all_keys, min(max_points, len(all_keys)))

        else:
            raise ValueError(f"æœªçŸ¥çš„ä¸‹é‡‡æ ·æ–¹æ³•: {method}")

        # åˆ›å»ºä¸‹é‡‡æ ·åçš„å­—å…¸
        downsampled_features = {key: point_features[key] for key in selected_keys}

        final_count = len(downsampled_features)
        reduction_ratio = (original_count - final_count) / original_count * 100

        print(f"   ä¸‹é‡‡æ ·å®Œæˆ: {original_count} -> {final_count} ç‚¹ (å‡å°‘ {reduction_ratio:.1f}%)")

        return downsampled_features

    def query_and_highlight(self, text_query: str, threshold: Optional[float] = None) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:
        """
        è¯­è¨€æŸ¥è¯¢é˜¶æ®µï¼šæ ¹æ®æ–‡æœ¬æŸ¥è¯¢é«˜äº®3Dç‚¹

        Args:
            text_query: æŸ¥è¯¢æ–‡æœ¬
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼

        Returns:
            (highlighted_points, highlighted_colors, stats)
        """
        if not self.feature_cache:
            raise ValueError("ç‰¹å¾ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨precompute_features()")

        if threshold is None:
            threshold = self.similarity_threshold

        print(f"ğŸ” è¯­è¨€æŸ¥è¯¢: '{text_query}' (é˜ˆå€¼: {threshold:.3f})")
        start_time = time.time()

        # ç¼–ç æ–‡æœ¬æŸ¥è¯¢
        text_features = self._encode_text_query(text_query)

        # è®¡ç®—ç›¸ä¼¼åº¦å¹¶ç­›é€‰ç‚¹
        highlighted_points, highlighted_colors, stats = self._compute_similarity_and_filter(
            text_features, threshold
        )

        query_time = time.time() - start_time
        print(f"âœ… æŸ¥è¯¢å®Œæˆ: {len(highlighted_points)} ä¸ªé«˜äº®ç‚¹ ({query_time:.3f}s)")

        return highlighted_points, highlighted_colors, stats

    def _encode_text_query(self, text_query: str) -> torch.Tensor:
        """ç¼–ç æ–‡æœ¬æŸ¥è¯¢"""
        with torch.no_grad():
            # ä½¿ç”¨CLIP_Surgeryçš„æ–‡æœ¬ç¼–ç 
            text_features = clip.encode_text_with_prompt_ensemble(
                self.clip_model, [text_query], self.device
            )
            text_features = text_features[0]  # å–ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªï¼‰æ–‡æœ¬çš„ç‰¹å¾

            print(f"   ğŸ“ æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {text_features.shape}")

            return text_features

    def _compute_similarity_and_filter(self, text_features: torch.Tensor, threshold: float) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:
        """è®¡ç®—ç›¸ä¼¼åº¦å¹¶è¿‡æ»¤é«˜äº®ç‚¹"""
        point_features = self.feature_cache['point_features']

        highlighted_points = []
        highlighted_scores = []
        total_points = len(point_features)
        processed_points = 0

        print(f"   ğŸ”„ è®¡ç®— {total_points} ä¸ª3Dç‚¹çš„patchç›¸ä¼¼åº¦...")

        # é€ç‚¹è®¡ç®—ç›¸ä¼¼åº¦
        for point_key, patch_feature in point_features.items():
            # è§£æ3Dåæ ‡
            coords = [float(x) for x in point_key.split(',')]
            point_3d = np.array(coords)

            # å½’ä¸€åŒ–patchç‰¹å¾
            patch_feature_norm = patch_feature / patch_feature.norm(dim=-1, keepdim=True)

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = torch.dot(text_features, patch_feature_norm).item()

            # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œæ·»åŠ åˆ°é«˜äº®åˆ—è¡¨
            if similarity >= threshold:
                highlighted_points.append(point_3d)
                highlighted_scores.append(similarity)

            processed_points += 1
            if processed_points % 10000 == 0:
                print(f"   å·²å¤„ç† {processed_points}/{total_points} ä¸ªç‚¹ï¼Œå½“å‰é«˜äº®ç‚¹æ•°: {len(highlighted_points)}")

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if highlighted_points:
            highlighted_points = np.array(highlighted_points)
            highlighted_scores = np.array(highlighted_scores)

            # ç”Ÿæˆé¢œè‰²ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰
            highlighted_colors = self._generate_highlight_colors(highlighted_scores)
        else:
            highlighted_points = np.empty((0, 3))
            highlighted_colors = np.empty((0, 3))
            highlighted_scores = np.array([])

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_points': total_points,
            'highlighted_points': len(highlighted_points),
            'threshold': threshold,
            'max_similarity': float(np.max(highlighted_scores)) if len(highlighted_scores) > 0 else 0.0,
            'min_similarity': float(np.min(highlighted_scores)) if len(highlighted_scores) > 0 else 0.0,
            'mean_similarity': float(np.mean(highlighted_scores)) if len(highlighted_scores) > 0 else 0.0,
            'matching_method': 'clip_surgery_patch'
        }

        return highlighted_points, highlighted_colors, stats

    def _generate_highlight_colors(self, scores: np.ndarray) -> np.ndarray:
        """æ ¹æ®ç›¸ä¼¼åº¦åˆ†æ•°ç”Ÿæˆé«˜äº®é¢œè‰²"""
        if len(scores) == 0:
            return np.empty((0, 3))

        # å½’ä¸€åŒ–åˆ†æ•°åˆ°[0, 1]
        min_score = np.min(scores)
        max_score = np.max(scores)

        if max_score > min_score:
            normalized_scores = (scores - min_score) / (max_score - min_score)
        else:
            normalized_scores = np.ones_like(scores)

        # ç”Ÿæˆé¢œè‰²ï¼šä»çº¢è‰²åˆ°é»„è‰²æ¸å˜
        colors = []
        base_color = np.array(self.highlight_color)  # é»˜è®¤é»„è‰²

        for score in normalized_scores:
            # åŸºäºåˆ†æ•°è°ƒæ•´é¢œè‰²å¼ºåº¦
            intensity = 0.5 + 0.5 * score  # 0.5åˆ°1.0çš„å¼ºåº¦
            color = base_color * intensity
            colors.append(color.astype(np.uint8))

        return np.array(colors)

    def clear_cache(self):
        """æ¸…ç†å†…å­˜ç¼“å­˜"""
        self.feature_cache.clear()
        self.scene_hash = None

        # æ¸…ç†GPUå†…å­˜
        if self.device.startswith('cuda'):
            torch.cuda.empty_cache()
            gc.collect()

        print("ğŸ§¹ CLIP_Surgeryç‰¹å¾ç¼“å­˜å·²æ¸…ç†")

    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        info = {
            'model_name': self.model_name,
            'scene_hash': self.scene_hash,
            'has_features': bool(self.feature_cache),
            'device': self.device
        }

        if self.feature_cache:
            point_features = self.feature_cache.get('point_features', {})
            info.update({
                'num_3d_points': len(point_features),
                'num_views': self.feature_cache.get('predictions_meta', {}).get('num_views', 0)
            })

        return info


class ClipSurgeryViserIntegration:
    """CLIP_Surgeryä¸Viserçš„é›†æˆç±»"""

    def __init__(self, viser_server, clip_manager: ClipSurgeryFeatureManager, scene_center=None):
        """
        åˆå§‹åŒ–Viseré›†æˆ

        Args:
            viser_server: ViseræœåŠ¡å™¨å®ä¾‹
            clip_manager: CLIP_Surgeryç‰¹å¾ç®¡ç†å™¨
            scene_center: åœºæ™¯ä¸­å¿ƒåæ ‡ï¼Œç”¨äºåæ ‡å¯¹é½
        """
        self.viser_server = viser_server
        self.clip_manager = clip_manager
        self.highlight_objects = []  # å­˜å‚¨é«˜äº®å¯¹è±¡çš„å¼•ç”¨
        self.scene_center = scene_center if scene_center is not None else np.array([0, 0, 0])
        self.predictions = None  # å­˜å‚¨é¢„æµ‹æ•°æ®

        # æ¸…é™¤åœºæ™¯ä¸­å¯èƒ½å­˜åœ¨çš„æ—§é«˜äº®å¯¹è±¡
        self._clear_existing_highlights()

        # è®¾ç½®GUIæ§ä»¶
        self.setup_gui()

        print("âœ… CLIP_Surgery-Viseré›†æˆåˆå§‹åŒ–å®Œæˆ")
        if scene_center is not None:
            print(f"   åœºæ™¯ä¸­å¿ƒ: [{self.scene_center[0]:.3f}, {self.scene_center[1]:.3f}, {self.scene_center[2]:.3f}]")

    def _clear_existing_highlights(self):
        """æ¸…é™¤åœºæ™¯ä¸­å¯èƒ½å­˜åœ¨çš„æ—§é«˜äº®å¯¹è±¡"""
        if not self.viser_server:
            return

        try:
            # å°è¯•æ¸…é™¤æ‰€æœ‰åŒ…å« "clip_highlight" çš„èŠ‚ç‚¹
            if hasattr(self.viser_server.scene, '_nodes'):
                nodes_to_remove = []
                for path in self.viser_server.scene._nodes.keys():
                    if "clip_highlight" in path:
                        nodes_to_remove.append(path)

                removed_count = 0
                for path in nodes_to_remove:
                    try:
                        self.viser_server.scene._nodes[path].remove()
                        removed_count += 1
                    except Exception:
                        pass

                if removed_count > 0:
                    print(f"ğŸ§¹ æ¸…é™¤äº† {removed_count} ä¸ªæ—§çš„é«˜äº®å¯¹è±¡")

        except Exception as e:
            print(f"âš ï¸ æ¸…é™¤æ—§é«˜äº®å¯¹è±¡æ—¶å‡ºç°é—®é¢˜: {e}")

    def setup_gui(self):
        """è®¾ç½®Viser GUIæ§ä»¶"""
        if not self.viser_server:
            return

        # æ–‡æœ¬è¾“å…¥æ¡†
        self.gui_text_query = self.viser_server.gui.add_text(
            "Query Text",
            initial_value="red chair"
        )

        # ç›¸ä¼¼åº¦é˜ˆå€¼æ»‘å—
        self.gui_similarity_threshold = self.viser_server.gui.add_slider(
            "Similarity Threshold",
            min=0.0,
            max=1.0,
            step=0.01,
            initial_value=self.clip_manager.similarity_threshold
        )

        # é«˜äº®æŒ‰é’®
        self.gui_highlight_button = self.viser_server.gui.add_button("Highlight Objects")

        # æ¸…é™¤é«˜äº®æŒ‰é’®
        self.gui_clear_button = self.viser_server.gui.add_button("Clear Highlights")

        # çŠ¶æ€æ˜¾ç¤º
        self.gui_status = self.viser_server.gui.add_text(
            "Status",
            initial_value="Ready for highlighting (CLIP_Surgery)",
            disabled=True
        )

        # ç»‘å®šäº‹ä»¶
        @self.gui_highlight_button.on_click
        def _(_):
            self.perform_highlight()

        @self.gui_clear_button.on_click
        def _(_):
            self.clear_highlights()

    def perform_highlight(self):
        """æ‰§è¡Œé«˜äº®æ“ä½œ"""
        try:
            # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å·²é¢„è®¡ç®—
            if not self.clip_manager.feature_cache:
                self.gui_status.value = "Error: No features computed. Please run reconstruction first."
                return

            # è·å–æŸ¥è¯¢å‚æ•°
            text_query = self.gui_text_query.value.strip()
            threshold = self.gui_similarity_threshold.value

            if not text_query:
                self.gui_status.value = "Error: Please enter a query text."
                return

            self.gui_status.value = f"Highlighting '{text_query}' (CLIP_Surgery)..."

            # æ‰§è¡ŒæŸ¥è¯¢
            highlighted_points, highlighted_colors, stats = self.clip_manager.query_and_highlight(
                text_query, threshold
            )

            # æ¸…é™¤ä¹‹å‰çš„é«˜äº®
            self.clear_highlights()

            # æ˜¾ç¤ºé«˜äº®ç»“æœ
            if len(highlighted_points) > 0:
                self.visualize_highlights(highlighted_points, highlighted_colors, text_query)

                # æ›´æ–°çŠ¶æ€
                self.gui_status.value = (
                    f"Found {len(highlighted_points)} points "
                    f"(max: {stats['max_similarity']:.3f}, "
                    f"mean: {stats['mean_similarity']:.3f})"
                )
            else:
                self.gui_status.value = f"No points found for '{text_query}' (threshold: {threshold:.3f})"

        except Exception as e:
            error_msg = f"Error: {str(e)}"
            self.gui_status.value = error_msg
            print(f"âŒ é«˜äº®æ“ä½œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def visualize_highlights(self, points: np.ndarray, colors: np.ndarray, query_text: str):
        """åœ¨Viserä¸­å¯è§†åŒ–é«˜äº®ç‚¹"""
        try:
            # åº”ç”¨åœºæ™¯ä¸­å¿ƒåŒ–ï¼Œä¸åŸºç¡€ç‚¹äº‘ä¿æŒä¸€è‡´
            centered_points = points - self.scene_center

            # æ·»åŠ é«˜äº®ç‚¹äº‘
            highlight_cloud = self.viser_server.scene.add_point_cloud(
                f"/clip_highlight_{query_text}",
                points=centered_points.astype(np.float32),
                colors=colors,
                point_size=self.clip_manager.point_size
            )
            self.highlight_objects.append(highlight_cloud)

            # æ·»åŠ æ ‡ç­¾ï¼ˆåœ¨ç‚¹äº‘ä¸­å¿ƒä¸Šæ–¹ï¼‰
            if len(centered_points) > 0:
                center = np.mean(centered_points, axis=0)
                label_position = center + np.array([0, 0, 0.1])  # ç¨å¾®ä¸Šç§»

                label = self.viser_server.scene.add_label(
                    f"/clip_highlight_{query_text}/label",
                    text=f"'{query_text}' ({len(points)} points)",
                    position=label_position
                )
                self.highlight_objects.append(label)

            print(f"âœ… é«˜äº®å¯è§†åŒ–å®Œæˆ: {len(points)} ä¸ªç‚¹")

        except Exception as e:
            print(f"âŒ é«˜äº®å¯è§†åŒ–å¤±è´¥: {e}")

    def clear_highlights(self):
        """æ¸…é™¤æ‰€æœ‰é«˜äº®æ˜¾ç¤º"""
        try:
            # æ¸…é™¤å½“å‰ä¼šè¯ä¸­è·Ÿè¸ªçš„é«˜äº®å¯¹è±¡
            removed_count = 0
            for obj in self.highlight_objects:
                try:
                    obj.remove()
                    removed_count += 1
                except Exception:
                    pass

            self.highlight_objects.clear()

            # é¢å¤–æ¸…é™¤ï¼šå°è¯•ç§»é™¤å¯èƒ½é—ç•™çš„é«˜äº®å¯¹è±¡
            try:
                if hasattr(self.viser_server.scene, '_nodes'):
                    nodes_to_remove = []
                    for path in self.viser_server.scene._nodes.keys():
                        if "clip_highlight" in path:
                            nodes_to_remove.append(path)

                    for path in nodes_to_remove:
                        try:
                            self.viser_server.scene._nodes[path].remove()
                            removed_count += 1
                        except Exception:
                            pass
            except Exception:
                pass

            self.gui_status.value = "Highlights cleared"
            if removed_count > 0:
                print(f"ğŸ§¹ é«˜äº®æ˜¾ç¤ºå·²æ¸…é™¤ ({removed_count} ä¸ªå¯¹è±¡)")

        except Exception as e:
            print(f"âš ï¸ æ¸…é™¤é«˜äº®å¤±è´¥: {e}")

    def update_features(self, predictions: Dict[str, Any]):
        """æ›´æ–°ç‰¹å¾ï¼ˆåœ¨é‡å»ºå®Œæˆåè°ƒç”¨ï¼‰"""
        try:
            # å­˜å‚¨é¢„æµ‹æ•°æ®
            self.predictions = predictions

            self.gui_status.value = "Computing CLIP_Surgery features..."

            # é¢„è®¡ç®—ç‰¹å¾
            scene_hash = self.clip_manager.precompute_features(predictions)

            # æ›´æ–°çŠ¶æ€
            cache_info = self.clip_manager.get_cache_info()
            if cache_info.get('has_features', False):
                self.gui_status.value = (
                    f"CLIP_Surgery ready: {cache_info.get('num_3d_points', 0)} points, "
                    f"{cache_info.get('num_views', 0)} views"
                )
            else:
                self.gui_status.value = "CLIP_Surgery features ready"

            print(f"âœ… CLIP_Surgeryç‰¹å¾æ›´æ–°å®Œæˆ: {scene_hash}")

        except Exception as e:
            error_msg = f"Feature computation failed: {str(e)}"
            self.gui_status.value = error_msg
            print(f"âŒ ç‰¹å¾æ›´æ–°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def get_device():
    """è·å–è®¡ç®—è®¾å¤‡"""
    if torch.cuda.is_available():
        visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', None)
        if visible_devices is not None:
            device = "cuda:0"
        else:
            device = "cuda"

        current_device = torch.cuda.current_device()
        device_name = torch.cuda.get_device_name(current_device)
        print(f"ğŸš€ ä½¿ç”¨GPUè®¾å¤‡: {device} ({device_name})")
        return device
    else:
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        return "cpu"


def load_vggt_model(device):
    """åŠ è½½VGGTæ¨¡å‹"""
    print("ğŸ”„ æ­£åœ¨åŠ è½½VGGTæ¨¡å‹...")
    start_time = time.time()

    model = VGGT.from_pretrained("/data22/ljc/proj/ckpt/VGGT-1B")
    model.eval()
    model = model.to(device)

    load_time = time.time() - start_time
    print(f"âœ… VGGTæ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}s)")

    return model


def load_or_reconstruct_scene(data_dir: str, model, device):
    """åŠ è½½æˆ–é‡å»ºåœºæ™¯"""
    data_path = Path(data_dir)

    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„å­˜çš„é‡å»ºç»“æœ
    predictions_file = data_path / "vggt_predictions.pkl"

    if predictions_file.exists():
        print(f"ğŸ“‚ åŠ è½½é¢„å­˜çš„é‡å»ºç»“æœ: {predictions_file}")
        with open(predictions_file, 'rb') as f:
            predictions = pickle.load(f)
        print("âœ… é‡å»ºç»“æœåŠ è½½å®Œæˆ")
        return predictions

    # å¦‚æœæ²¡æœ‰é¢„å­˜ç»“æœï¼Œè¿›è¡Œé‡å»º
    print(f"ğŸ”„ å¼€å§‹é‡å»ºåœºæ™¯: {data_dir}")

    # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
    image_extensions = ['.jpg', '.jpeg', '.png', '.HEIC']
    image_files = []

    for ext in image_extensions:
        image_files.extend(list(data_path.glob(f"**/*{ext}")))
        image_files.extend(list(data_path.glob(f"**/*{ext.upper()}")))

    if not image_files:
        raise ValueError(f"åœ¨ {data_dir} ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")

    # æ’åºå¹¶é™åˆ¶æ•°é‡ï¼ˆæ¼”ç¤ºç”¨ï¼‰
    image_files = sorted(image_files)[:20]  # æœ€å¤š20å¼ å›¾ç‰‡
    image_paths = [str(f) for f in image_files]

    print(f"ğŸ“· æ‰¾åˆ° {len(image_paths)} å¼ å›¾åƒ")

    # é¢„å¤„ç†å›¾åƒ
    print("ğŸ”„ é¢„å¤„ç†å›¾åƒ...")
    images = load_and_preprocess_images(image_paths)
    images = images.to(device)

    # æ£€æŸ¥å¹¶ä¿®æ­£å›¾åƒå¼ é‡ç»´åº¦
    print(f"ğŸ“ åŸå§‹å›¾åƒå¼ é‡å½¢çŠ¶: {images.shape}")
    if len(images.shape) == 4:  # (S, C, H, W)
        # VGGTæœŸæœ› (B, S, C, H, W) æ ¼å¼ï¼Œæ·»åŠ batchç»´åº¦
        images = images.unsqueeze(0)  # (1, S, C, H, W)
        print(f"ğŸ“ ä¿®æ­£åå›¾åƒå¼ é‡å½¢çŠ¶: {images.shape}")

    # VGGTé‡å»º
    print("ğŸ”„ VGGTé‡å»ºä¸­...")
    start_time = time.time()

    with torch.no_grad():
        predictions = model(images)

    reconstruction_time = time.time() - start_time
    print(f"âœ… VGGTé‡å»ºå®Œæˆ ({reconstruction_time:.2f}s)")

    # è½¬æ¢å§¿æ€ç¼–ç ä¸ºå¤–å‚å’Œå†…å‚çŸ©é˜µ
    print("ğŸ”„ è½¬æ¢å§¿æ€ç¼–ç ...")
    from vggt.utils.pose_enc import pose_encoding_to_extri_intri
    extrinsic, intrinsic = pose_encoding_to_extri_intri(predictions["pose_enc"], images.shape[-2:])
    predictions["extrinsic"] = extrinsic
    predictions["intrinsic"] = intrinsic

    # å¤„ç†æ¨¡å‹è¾“å‡ºï¼šç§»é™¤batchç»´åº¦å¹¶è½¬æ¢ä¸ºnumpy
    print("ğŸ”„ å¤„ç†æ¨¡å‹è¾“å‡º...")
    for key in predictions.keys():
        if isinstance(predictions[key], torch.Tensor):
            predictions[key] = predictions[key].cpu().numpy().squeeze(0)  # ç§»é™¤batchç»´åº¦

    print(f"ğŸ“ å¤„ç†åçš„æ•°æ®å½¢çŠ¶:")
    for key, value in predictions.items():
        if hasattr(value, 'shape'):
            print(f"  {key}: {value.shape}")

    # ä¿å­˜é‡å»ºç»“æœ
    try:
        with open(predictions_file, 'wb') as f:
            pickle.dump(predictions, f)
        print(f"ğŸ’¾ é‡å»ºç»“æœå·²ä¿å­˜: {predictions_file}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜é‡å»ºç»“æœå¤±è´¥: {e}")

    return predictions


def visualize_basic_scene(viser_server, predictions):
    """å¯è§†åŒ–åŸºç¡€åœºæ™¯ï¼ˆç‚¹äº‘å’Œç›¸æœºï¼‰"""
    print("ğŸ¨ å¯è§†åŒ–åŸºç¡€åœºæ™¯...")

    # ç”Ÿæˆç‚¹äº‘
    world_points = unproject_depth_map_to_point_map(
        predictions["depth"], predictions["extrinsic"], predictions["intrinsic"]
    )

    # è·å–å›¾åƒé¢œè‰²
    images = predictions["images"]  # (S, 3, H, W)
    colors = images.transpose(0, 2, 3, 1)  # (S, H, W, 3)

    S, H, W, _ = world_points.shape

    # é‡å¡‘ä¸ºå¹³é¢æ•°æ®
    points_flat = world_points.reshape(-1, 3)
    colors_flat = (colors.reshape(-1, 3) * 255).astype(np.uint8)

    # è¿‡æ»¤æœ‰æ•ˆç‚¹
    valid_mask = ~np.isnan(points_flat).any(axis=1)
    valid_mask &= ~np.isinf(points_flat).any(axis=1)

    valid_points = points_flat[valid_mask]
    valid_colors = colors_flat[valid_mask]

    if len(valid_points) > 0:
        # è®¡ç®—åœºæ™¯ä¸­å¿ƒå¹¶ä¸­å¿ƒåŒ–
        scene_center = np.mean(valid_points, axis=0)
        centered_points = valid_points - scene_center

        # æ·»åŠ ç‚¹äº‘
        point_cloud = viser_server.scene.add_point_cloud(
            "/point_cloud",
            points=centered_points.astype(np.float32),
            colors=valid_colors,
            point_size=0.001
        )

        print(f"âœ… å·²æ·»åŠ ç‚¹äº‘: {len(centered_points)} ä¸ªç‚¹")

        # æ·»åŠ ç›¸æœºå¯è§†åŒ–
        visualize_cameras(viser_server, predictions, scene_center)

        return scene_center
    else:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„ç‚¹äº‘æ•°æ®")
        return np.array([0, 0, 0])


def visualize_cameras(viser_server, predictions, scene_center):
    """å¯è§†åŒ–ç›¸æœºä½ç½®å’Œå›¾åƒ"""
    print("ğŸ“· æ·»åŠ ç›¸æœºå¯è§†åŒ–...")

    # è·å–ç›¸æœºå‚æ•°
    extrinsics = predictions["extrinsic"]  # (S, 3, 4)
    images = predictions["images"]  # (S, 3, H, W)

    # è½¬æ¢ç›¸æœºå¤–å‚ä¸ºä¸–ç•Œåæ ‡
    cam_to_world_mat = closed_form_inverse_se3(extrinsics)
    cam_to_world = cam_to_world_mat[:, :3, :]

    # åº”ç”¨åœºæ™¯ä¸­å¿ƒåŒ–
    cam_to_world[..., -1] -= scene_center

    frames = []
    frustums = []

    def attach_callback(frustum, frame):
        """ä¸ºç›¸æœºè§†é”¥ä½“æ·»åŠ ç‚¹å‡»å›è°ƒ"""
        @frustum.on_click
        def _(_):
            for client in viser_server.get_clients().values():
                client.camera.wxyz = frame.wxyz
                client.camera.position = frame.position

    # ä¸ºæ¯ä¸ªç›¸æœºæ·»åŠ å¯è§†åŒ–
    for img_id in range(len(images)):
        cam2world_3x4 = cam_to_world[img_id]
        T_world_camera = viser.transforms.SE3.from_matrix(cam2world_3x4)

        # æ·»åŠ ç›¸æœºåæ ‡è½´
        frame_axis = viser_server.scene.add_frame(
            f"/cameras/frame_{img_id}",
            wxyz=T_world_camera.rotation().wxyz,
            position=T_world_camera.translation(),
            axes_length=0.05,
            axes_radius=0.002,
            origin_radius=0.002,
        )
        frames.append(frame_axis)

        # è½¬æ¢å›¾ç‰‡æ ¼å¼
        img = images[img_id]  # (3, H, W)
        img = (img.transpose(1, 2, 0) * 255).astype(np.uint8)  # (H, W, 3)
        h, w = img.shape[:2]

        # è®¡ç®—FOV
        fy = 1.1 * h
        fov = 2 * np.arctan2(h / 2, fy)

        # æ·»åŠ è§†é”¥ä½“
        frustum_cam = viser_server.scene.add_camera_frustum(
            f"/cameras/frame_{img_id}/frustum",
            fov=fov,
            aspect=w / h,
            scale=0.05,
            image=img,
            line_width=1.0
        )
        frustums.append(frustum_cam)
        attach_callback(frustum_cam, frame_axis)

    print(f"âœ… å·²æ·»åŠ  {len(frames)} ä¸ªç›¸æœº")

    # æ·»åŠ GUIæ§åˆ¶
    gui_show_cameras = viser_server.gui.add_checkbox("Show Cameras", initial_value=True)

    @gui_show_cameras.on_update
    def _(_):
        for frame in frames:
            frame.visible = gui_show_cameras.value
        for frustum in frustums:
            frustum.visible = gui_show_cameras.value

    return frames, frustums


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="CLIP_Surgeryè¯­è¨€å¼•å¯¼3Dé«˜äº®æ¼”ç¤º")
    parser.add_argument("--data_dir", type=str, required=True, help="é‡å»ºæ•°æ®ç›®å½•")
    parser.add_argument("--port", type=int, default=6077, help="ViseræœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="ViseræœåŠ¡å™¨ä¸»æœº")
    parser.add_argument("--model", type=str, default="CS-ViT-B/16", help="CLIP_Surgeryæ¨¡å‹")

    args = parser.parse_args()

    print("ğŸš€ CLIP_Surgeryè¯­è¨€å¼•å¯¼3Dé«˜äº®æ¼”ç¤ºå¯åŠ¨")
    print(f"ğŸ“ æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"ğŸŒ æœåŠ¡å™¨: {args.host}:{args.port}")
    print(f"ğŸ¤– CLIP_Surgeryæ¨¡å‹: {args.model}")

    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(args.data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        return

    # è·å–è®¾å¤‡
    device = get_device()

    # åŠ è½½VGGTæ¨¡å‹
    vggt_model = load_vggt_model(device)

    # åŠ è½½æˆ–é‡å»ºåœºæ™¯
    predictions = load_or_reconstruct_scene(args.data_dir, vggt_model, device)

    # å¯åŠ¨ViseræœåŠ¡å™¨
    print(f"ğŸš€ å¯åŠ¨ViseræœåŠ¡å™¨: {args.host}:{args.port}")
    viser_server = viser.ViserServer(host=args.host, port=args.port)
    viser_server.gui.configure_theme(titlebar_content=None, control_layout="collapsible")

    # å¯è§†åŒ–åŸºç¡€åœºæ™¯
    scene_center = visualize_basic_scene(viser_server, predictions)

    # åˆå§‹åŒ–CLIP_Surgeryç‰¹å¾ç®¡ç†å™¨
    print("ğŸ”„ åˆå§‹åŒ–CLIP_Surgeryç‰¹å¾ç®¡ç†å™¨...")
    clip_manager = ClipSurgeryFeatureManager(model_name=args.model)

    # åˆå§‹åŒ–CLIP_Surgery-Viseré›†æˆ
    print("ğŸ”„ åˆå§‹åŒ–CLIP_Surgery-Viseré›†æˆ...")
    clip_integration = ClipSurgeryViserIntegration(viser_server, clip_manager, scene_center=scene_center)

    # é¢„è®¡ç®—CLIP_Surgeryç‰¹å¾
    print("ğŸ”„ é¢„è®¡ç®—CLIP_Surgeryç‰¹å¾...")
    clip_integration.update_features(predictions)

    print(f"âœ… CLIP_Surgeryæ¼”ç¤ºç³»ç»Ÿå¯åŠ¨å®Œæˆ!")
    print(f"ğŸŒ è¯·è®¿é—®: http://{args.host}:{args.port}")
    if args.host == "0.0.0.0":
        print(f"ğŸŒ æˆ–è®¿é—®: http://localhost:{args.port}")
    print(f"ğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print(f"   1. åœ¨'Query Text'ä¸­è¾“å…¥æŸ¥è¯¢æ–‡æœ¬ï¼ˆå¦‚'red chair', 'plant', 'table'ï¼‰")
    print(f"   2. è°ƒæ•´'Similarity Threshold'æ»‘å—è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼")
    print(f"   3. ç‚¹å‡»'Highlight Objects'æŒ‰é’®è¿›è¡Œé«˜äº®")
    print(f"   4. ç‚¹å‡»'Clear Highlights'æŒ‰é’®æ¸…é™¤é«˜äº®")
    print(f"ğŸ”§ æŒ‰Ctrl+Cé€€å‡º")

    try:
        # ä¿æŒæœåŠ¡å™¨è¿è¡Œ
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ CLIP_Surgeryæ¼”ç¤ºç³»ç»Ÿå…³é—­")


if __name__ == "__main__":
    main()
